---
title: "Day 01 -- Hypothesis & the Linear Model"
description: "Hypothesis Testing on the context of OLS"
output: 
  learnr::tutorial:
    fig_caption: no
    progressive: true
    allow_skip: true
    toc: true
    toc_depth: 3
    theme: readable
runtime: shiny_prerendered
editor_options: 
  markdown: 
    wrap: 80
---

```{r setup, include=FALSE}
library(learnr)
library(gradethis)

knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)

knitr::knit_engines$set("html") # for linux machines

source("../R/helper_code.R")

# Check whether required packages are installed
pkgs <- matrix(c(
  "learnr", "0.11.5", "CRAN",
  "gradethis", "0.2.14", "rstudio/gradethis",
  "outliertree", "1.9.0", "CRAN"
), byrow = TRUE, ncol = 3) |> 
  as.data.frame() |> 
  setNames(c("pkg", "version", "where"))

check_pkgs <- function(.pkgs = pkgs) {
  bootcamp:::check_packages(.pkgs)
}


check_RStudio <- function() {
  bootcamp::check_rstudio_equal_or_larger("2024.04.2", verdict = TRUE)
}

check_R <- function() {
  bootcamp::check_r_equal_or_larger("4.4.1", verdict = TRUE)
}

```

```{css, echo = FALSE}
.tip {
  border-radius: 10px;
  padding: 10px;
  border: 2px solid #136CB9;
  background-color: #136CB9;
  background-color: rgba(19, 108, 185, 0.1);
  color: #2C5577;
}

.warning {
  border-radius: 10px;
  padding: 10px;
  border: 2px solid #f3e2c4;
  background-color: #f3e2c4;
  background-color: rgba(243, 226, 196, 0.1);
  color: #775418;
}

.infobox {
  border-radius: 10px;
  padding: 10px;
  border: 2px solid #868e96;
  background-color: #868e96;
  background-color: rgba(134, 142, 150, 0.1);
  color: #2F4F4F;
}

# # create a horizontal scroll bar when code is too wide
# pre, code {white-space:pre !important; overflow-x:auto}
```

```{html, echo = FALSE}
<style>
pre {
  white-space: pre-wrap;
  background: #F5F5F5;
  max-width: 100%;
  overflow-x: auto;
}
</style>
```

## Introduction

In this tutorial, you are going to learn how to do OLS in `r rproj()`.

While you are at it, you'll learn a couple of additional tricks and will get to
solidify your understanding of the stuff that was discussed in the lecture.

Let's start doing the heavy lifting in this bootcamp.

Pure fun!

![](images/weight_lift.gif)

## Checking installation

Before we run these tutorials, we first quickly make sure you have all of the
required packages installed for this tutorial.

### R Version

You need to have installed R version 4.2.1 or higher and this tutorial is going
check for you if you have at least that version. Please hit the `Run Code`
button.

```{r r_check, echo = TRUE, include = TRUE, exercise = TRUE}
check_R()
```

### R Studio Version

You need to have installed RStudio version 2022.07.1 or above. Let's check if
you are using this by clicking `Run Code`:

```{r rstudio_check, echo = TRUE, include = TRUE, exercise = TRUE}
check_RStudio()
```

### Packages

You need to have a few packages installed. Click the `Run Code` to check. It
will check whether you have the required packages installed and will attempt to
install any missing packages in case there are any.

```{r package_check, echo = TRUE, include = TRUE, exercise = TRUE}
check_pkgs()
```


## Some notes about R

We are going to work with the exceptionally cool and powerful `r rproj()` 
language. Yes, other programming languages exist that are also cool, but 
`r rproj()` is by far the best (and most flexible) when it comes to statistical 
modeling. Any good data scientist over the next decades will need to be 
multi-lingual: they need to master multiple languages. These will typically 
include `r rproj()` and Python, and in the near future probably alsi Julia. 
In addition, masting C++ also helps greatly. And Javascript. 
If you go on to the labor market trying to get a job as a data scientist and 
only know one of these languages well, you will be at a disadvantage to others 
who did spend the effort to learn at least both `r rproj()` and Python. 

So, lots of work for you to develop these skills in the coming years.

At JADS, you will become trained in both `r rproj()` and Python and you will 
be using both languages starting this semester already. In this bootcamp, we will 
be using `r rproj()`, since that is by far the best language for statistical 
analysis. 

Now, we do realize that many of you may never have used `r rproj()` before. 
So, we are going to be light on your `r rproj()` skills and will focus mainly 
on your understanding of the statisticsm not so much of `r rproj()`. 
But you will need to be able to work with `r rproj()` just a little bit, so 
you can do the exercises and run analyses during the exam of coming Friday.... 

So, here are a few things to get you started.

When you use a function from a package, prefix that package name to the function 
you want to use. So, rather than typing

``` r
betterpairs(amazon)
```

you type:

``` r
bootcamp::betterpairs(amazon)
```

(you will use this in a few minutes)

There is no need to do this for functions that come packaged with `r rproj()`, 
as you will see in the next line of code below.

Also, when a function produces output, you can save that output in a new variable 
(in `r rproj()` this is called an *object*) using `<-`. For example:

``` r
mod_2 <- lm(after_sales ~ before_sales + logo, data = amazon)
```

Finally, if you want to access something that is inside an object, you use `$`. 
So, one way of accessing the variable `logo` that is inside `amazon` is


``` r
amazon$logo
```

For the rest, pay close attention to the code presented to you in this and the 
coming tutorials. We will slowly make you a proficient `r rproj()` 
programmer--a skill you will need in several courses and in much of your 
future career as a data scientist.

Of course, if you have a coding question (or a statistics question), feel free 
to come to Claudia or Roger and ask us to assist you. We'd be happy to do so.

## Guided example

Let's start with describing the data set.

The data we will work with is called "Amazon" and is included in bootcamp
package. Let's load it.

```{r load_amazon, include = FALSE}
amazon <- bootcamp::amazon
```

```{r amazon, echo = TRUE, include = TRUE, exercise = TRUE}
data(amazon, package = "bootcamp")
amazon
```

The data refer to experiments regarding three Amazon logos. The intent was to
check which logo would entice consumers to buy the most from Amazon: the
original logo or one of two others?

Let's look at the three logos:

![Amazon Logos](images/Amazon_logo_variations.png)

Which one makes you grab your wallet the most?<br> (can you spot the
differences? They are minor...)

### Modeling process

The general process of fitting linear model is as follows.

1.  First, we look at the data to see if it looks the way we expect. It also
    helps us to learn a bit about how they are distributed. Look at some numeric
    descriptives and some plots.<br> If we find any problems or see any
    surprising observations, we look at those in more detail (and repair any
    errors) before fitting an actual model.

2.  Second, we build the model we are interested in. Often, we will build
    multiple models and compare them.

3.  Third, we consider the diagnostics of the various models: we check the fit
    to the data and whether the underlying model assumptions appear to be
    supported.

### Look at the data

For further info about the dataset, check out the help for it.

```{r amazon_check, echo = TRUE, include = TRUE, exercise = TRUE, exercise.setup = "load_amazon"}
# look at the help file, depending on your browser this 
# will probably open the help in a separate browser tab
?bootcamp::amazon
?amazon
```

Now, we take a quick look at the data. In the bootcamp session on "data
descriptives" you will learn more about what to look for. So, for now, we'll
just do the minimal basics.

```{r amazon_summary, echo = TRUE, include = TRUE, exercise = TRUE, exercise.setup = "load_amazon"}
bootcamp::descriptives(amazon)
```

A quick plot to see the variable distributions and the correlations between the
variables:

```{r amazon_summary_plot, echo = TRUE, include = TRUE, exercise = TRUE, exercise.setup = "load_amazon"}
bootcamp::betterpairs(amazon)
```

And, finally, let's see if there are some odd observations:

```{r amazon_outliers, echo = TRUE, include = TRUE, exercise = TRUE, exercise.setup = "load_amazon"}
outliertree::outlier.tree(amazon)
```

Phew, nothing out of the ordinary, it looks like.

Let's start to build an OLS model.

## Building a formula

When you run any type of model in `r rproj()`, you need to tell it which
variables to include and how you want to use them: as linear additions,
interactions, transformations (logarithmic, power transforms, mean centered,
etc.), grouping, andsoforth.

Let's discuss how to do this.

### Formula syntax

The models fit by, e.g., the `lm` and `glm` functions are specified in a compact
symbolic form.

+-------------+-------------+------------------------------------------------+
| operator    | example     | meaning                                        |
+:===========:+:============+:===============================================+
| \~          | y \~ x      | Model y as a function of x                     |
+-------------+-------------+------------------------------------------------+
| \+          | y \~ a + b  | Include columns a as well as b                 |
+-------------+-------------+------------------------------------------------+
| \-          | y \~ a - b  | Include a but exclude b                        |
+-------------+-------------+------------------------------------------------+
| :           | y \~ a : b  | Estimate the interaction of a and b            |
+-------------+-------------+------------------------------------------------+
| \*          | y \~ a \* b | Include columns as well as their interaction   |
|             |             |                                                |
|             |             | (this is identical to y \~ a + b + a:b)        |
+-------------+-------------+------------------------------------------------+
| 1           | y \~ 1      | Include only the intercept, but no other       |
|             |             | variables                                      |
+-------------+-------------+------------------------------------------------+
| 1           | y \~ a +    | Exclude the intercept, include only a and b    |
|             | b - 1       |                                                |
+-------------+-------------+------------------------------------------------+
| \|          | y \~ a \| b | Estimate y as a function of a conditional on b |
+-------------+-------------+------------------------------------------------+
| .           | y \~ .      | Include all variables in the dataset           |
+-------------+-------------+------------------------------------------------+

### The `lm` function

You run OLS with the `lm` function (which is short for "*l*inear *m*odel"). The
usage is

``` r
lm(formula, data, subset, weights, na.action,
   method = "qr", model = TRUE, x = FALSE, y = FALSE, qr = TRUE,
   singular.ok = TRUE, contrasts = NULL, offset, ...)
```

but you really only need to consider this:

``` r
lm(formula, data, subset, na.action)
```

-   The `formula` part is where you include the formula in the way we just
    discussed.

-   `data` is the name of the dataset

-   `subset` allows you to fit the model on a subset of the observations, rather
    than to all of them

-   `na.action` helps you define what to do with missings (ie. `NA`'s) in your
    dataset. You need to do something with them, because the method doesn't know
    what to do with them otherwise.<br> Useful options include:

    -   `na.action = na.fail`. This is the default: you will get an error that
        there are missings in the data.
    -   `na.action = na.omit`. This will remove each observation entirely for
        which one of the variables in the model has a missing value. In other
        words, the model is run on complete cases only.
    -   also useful can be to write your own function about what needs to be
        done with missings.

Let's say we have a dataframe called `salesdata` that contains the variables
`sales`, `advertising`, `airplay`, and `popularity`. See if you can answer the
questions below correctly.

```{r quiz_formula}
quiz(
  question("How do you regress `sales` on `advertising` and `airplay`?",
    answer("lm(sales = advertising + airplay)"),
    answer("lm(sales ~ advertising + airplay)", message = "This DOES work if 'sales', 'advertising' and 'airplay' are all loaded in your workspace, but if they are not there, this will not work--so don't do it this way."),
    answer("lm(sales ~ advertising + airplay, data = salesdata)", correct = TRUE),
    answer("lm(sales ~ advertising * airplay, data = salesdata)")
  , allow_retry = TRUE),
  question("How do you regress `sales` on `advertising`, `airplay`, and `popularity`?<br>(check all that apply)",
    answer("lm(sales ~ . , data = salesdata)", correct = TRUE),
    answer("lm(sales ~ advertising + airplay + popularity, data = salesdata)", correct = TRUE),
    answer("lm(sales ~ advertising * airplay * popularity, data = salesdata)"),
    answer("lm(sales ~ salesdata)")
  , allow_retry = TRUE)
)
```

## Application to the dataset

OK, now we go back to the Amazon experiment and the `amazon` dataset.\
You know, the experiment where we find out how to separate you from as much
money as possible.

Run an OLS regression where you model `after_sales` as a function of only an
intercept. Save the result to an object called `mod_intcp` and then run
`summary` or `jtools::summ` on this to see a nice overview of the findings.

```{r, amazon_exc_1, exercise = TRUE, exercise.lines = 2, exercise.setup = "load_amazon"}


```

```{r, amazon_exc_1-hint-1}
# The intercept is denoted by '1'
```

```{r, amazon_exc_1-hint-2}
# Make sure to include the dataset name 'amazon'
```

```{r, amazon_exc_1-hint-3}
lm(after_sales ~ 1, data = amazon)
```

```{r, amazon_exc_1-solution}
mod_intcp <- lm(after_sales ~ 1, data = amazon)
summary(mod_intcp)
```

This is the simplest regression model possible. You could have predicted the
result before hitting the 'run' button. How? See the follow quiz question.

```{r quiz_ols_mean, echo = FALSE}
quiz(
  question("What is the mean of `after_sales` in the dataset?",
    answer("-577.48"),
    answer("587.48", correct = TRUE),
    answer("36.52"),
    answer("118"),
    correct = "Correct - in the intercept-only model, the regression is equal to the mean of the variable!",
    random_answer_order = TRUE,
    allow_retry = TRUE
  )
)
```

Now, run a model where you explain `after_sales` as a linear function of
`before_sales` and `logo`. Write the result to `mod_2` and use `summary` or
`jtools::summ`to see the results.

```{r, amazon_exc_2, exercise = TRUE, exercise.lines = 2, exercise.setup = "load_amazon"}

```

```{r, amazon_exc_2-solution}
mod_2 <- lm(after_sales ~ before_sales + logo, data = amazon)
summary(mod_2)
```

Hey, do you see that? The intercept is automatically included!<br> Being a very
professional statistics environment, `r rproj()` knows that the lower-level
should always be included. So, when you include, one or more variables,
`r rproj()` will add the intercept for you. You can override this by
specifying<br> `lm(after_sales ~ -1 + before_sales + logo, data = amazon)`<br>
but why would you?

### Interpretation

What does the coefficient of -249 for `logo2` mean?<br> Well, let's look at the
`logo` variable first.

```{r, amazon_logo, exercise = TRUE,exercise.setup = "load_amazon"}
# The summary:
summary(amazon$logo)

# Logo has the following class:
class(amazon$logo)

# is logo ordinal or nominal?
is.ordered(amazon$logo)

# what are the categories of 'logo'?
levels(amazon$logo)
```

So, logo is a factor with three values. There is no order, since it makes no
sense to claim that logo2 is larger or smaller than logo3. This is a purely
nominal variable, where the actual values 1, 2, 3 do not have any meaning beyond
distinguishing which logo the observation refers to.

Actually, we could have coded `logo` as -999, pi, 6666 or as "A", "B", "X" or as
"green", "blue", "orange_with_just_a\_hint_of_brown" or whatever. <br> The
values only encode the specific condition the consumer was in--different numbers
refer to different conditions.

In order to run a meaningful analysis, you need two dummies. Here, logo1 is the
"reference" category.

-   When an observation refers to logo1, both dummies are 0.

-   When an observation refers to logo2, the dummy for logo2 is 1 and the dummy
    for logo3 is 0.

-   When an observation refers to logo3, the dummy for logo2 is 0 and the dummy
    for logo3 is 1.

You do not need three dummies (in fact, this creates a mathematical problem--
although many machine learning algorithms neglect this issue), because all
conditions are fully defined by only 2 dummies. This is generally true, when a
factor (ordered or non-ordered) has *n* categories, you need exactly *n-1*
dummies.<br>You'll learn more about this later in the bootcamp.

::: tip
Anyway, you saw that the class of `logo` is `factor`. Our beloved software
program `r rproj()` is so smart, that it automatically turns the factor into two
dummies and uses those inside the model. Most other statistical softwares force
you to manually add dummies to the dataset and then use those. ***That is
silly!*** It only blows up the dataset and serves no purpose other than **you**
doing the work that the **software** should be doing for you. It is nice to work
with `r rproj()`, because it is there to serve you (rather than the other way
around). <br><br>

Feel free to applaud your computer now. No worries, nobody is watching you.<br>
:::

<br><br> <bold> [Yeah yeah, get on with it already! You were going to explain
what the coefficient of -249 for `logo2` means!]{style="background:yellow"}
</bold>

Well, when we have the old logo, the model we just estimated is

$$
\text{after_sales} = 768.60 - .02*\text{before_sales} -249.61*0 - 260.46*0 + error
$$ In the condition with logo2, we have $$
\text{after_sales} = 768.60 - .02*\text{before_sales} - 249.61*1 - 260.46*0 + error
$$ In the condition with logo3, we have $$
\text{after_sales} = 768.60 - .02*\text{before_sales} + - 249.61*0 - 260.46*1 + error
$$ Finally, we are in the position to draw our conclusion. Let's say we had a
consumer with `before_sales` of \$500. Then, this consumer is expected to spend
768.60 -.02*500 = 758.60 USD when being confronted with logo1 (= the original
logo). If this same consumer would always see logo 2, she would spend 768.60
-.02*500 - 249.61 = 508.99 USD on average.

This is less! That is 249.61 less, to be exact. Hmm, a familiar number, wouldn't
you say?

In other words, the coefficient for `logo2` is the difference in `after_sales`
of logo2 compared to logo1 (= the reference category). Similarly, the
coefficient for `logo3` is the difference in `after_sales` of logo2 compared to
reference category logo1.

```{r quiz_amazon_intrp}
quiz(
  question("How much do you expect a consumer to spend who is in the 'logo3' condition and spent a total of 100$ in the past?",
    answer("about 260"),
    answer("about -260"),
    answer("about 506", 
           correct = TRUE, 
           message = "768.60 -.02*100 - 260.46"),
    answer("about 248"),
    random_answer_order = TRUE,
    allow_retry = TRUE
  ),
  question("Is the effect of logo2 statistically significant (at 0.05)?",
    answer("yes", correct = TRUE, 
           message = "Of course, this depends on the significance level 
           you decided on before running the model. In this case, 
           you would need to use an extraordinarily small significance 
           level for this effect not to be statistically significant"),
    answer("no"),
    answer("strongly", message = "There is no such thing as 
           'weakly significant', 'strongly significant', 
           'barely significant', etc., an effect is simply
           statistically significant or not. People who say that an effect 
           is 'strongly significant' should relearn statistics from zero again
           and get their statistics diploma revoked. Bam, we said it!"),
    random_answer_order = TRUE,
    allow_retry = TRUE
  ),
  question("What does 2e-16 mean?",
    answer("0.0000000000000002", correct = TRUE, 
           message = "That is 15 zeroes"),
    answer(".216"),
    answer(".0002"),
    answer(".016"),
    random_answer_order = TRUE,
    allow_retry = TRUE
  ),
  question("At what significance level would we say that 'before_sales' 
           is statistically significant?",
    answer(".1621", correct = TRUE, 
           message = "This would occur for any level larger than 0.162"),
    answer(".10"),
    answer(".20"),
    answer(".2e-16"),
    random_answer_order = TRUE,
    allow_retry = TRUE
  ),
  question("What will be the expected difference in spending between logo2 and logo3?",
    answer("10.85 in favor of logo2", correct = TRUE, 
           message = "-249.61 + 260.46 = 10.85"),
    answer("10.85 in favor of logo3", message = "Almost there, but logo3 yields lower 
           sales than logo2, you can see that from the regression coefficients"),
    answer("+249.61"),
    answer("+260.46"),
    random_answer_order = TRUE,
    allow_retry = TRUE
  )
)
```

For this last question, you could reorder the levels of `logo` and make `logo2`
or `logo3` the reference category. You can then find the expected sales
difference in the `lm` output. But you can already tell from the current
estimates what the result is. The only advantage of running that new regression
model is that you can test whether the difference is statistically significant
(but a little bit of knowledge of statistical intuition already tells you that
this is not the case) .

### Model of change in sales

Now, let's put some things together.<br> We built a model of `after_sales`, but
this makes little sense: we are interested to see if the logo change leads to
increased (or decreased) sales. In other words, we need a new dependent
variable.<br> Below, do the following:

1.  Create a new variable inside the `amazon` dataset that is `after_sales` -
    `before_sales` and call it `change_sales`. Do this through:
    `amazon$change_sales <- amazon$after_sales - amazon$before_sales`

2.  Run an OLS model where you regress `change_sales` on `logo`, `country`,
    `age`, `sex`, and the interaction between `age` and `sex`. The `country`
    variable is a dummy, it is 0 for US customers and 1 for customers from the
    UK.

```{r, amazon_change, exercise = TRUE, exercise.lines = 6, exercise.setup = "load_amazon"}


```

```{r, amazon_change-hint-1}
amazon$change_sales <- amazon$after_sales - amazon$before_sales
```

```{r, amazon_change-hint-2}
# When using 'A*B' you get A, B, and B:C
logo + country + age*sex
# same thing, just a little longer
logo + country + age + sex + age:sex
```

```{r, amazon_change-solution}
# just one way of doing this
amazon$change_sales <- amazon$after_sales - amazon$before_sales
mod_change <- lm(change_sales ~ logo + country + age*sex, data = amazon)
summary(mod_change)
```

```{r quiz_amz_change}
quiz(
  question("What does the 'country1' coefficient mean?",
    answer("On average, the change in sales is $25 for UK customers"),
    answer("On average, the change in sales is $25 for US customers"),
    answer("On average, the change in sales is $25 less for UK customers than for US customers", correct = TRUE),
    answer("On average, the change in sales is $25 less for US customers than for UK customers", 
           message = "If not otherwise specified, the lowest value is the reference category, 
           this is why the variable is called 'country1' in the output. The -24.92 means that 
           country = 1 (which is the UK) has a lower predicted sales change than that of 
           the reference category (which is the US"),
    random_answer_order = TRUE,
    allow_retry = TRUE
  ),
  question("If we use a 5% significance level, would we conclude that the change in sales 
           differs between the UK and US customers at a statistically significant level?",
    answer("no", correct = TRUE, message = "The p-value of 0.07 is larger that the 
           significance level of 0.05, so we conclude that there is no 
           statistically significant difference."),
    answer("yes", message = "Make sure to compare the significance level with the p-value"),
    answer("There is not enough info in this table"),
    random_answer_order = TRUE,
    allow_retry = TRUE
  ),
  question("Suppose that we want to test the hypothesis that the change in sales 
           for UK customers is smaller than for US customers. We test using 
           a significance level of .05.<br>Do we reject our null hypothesis (based 
           on the results above) or not?",
    answer("yes", correct = TRUE, message = "Great work! This is a single-sided 
           hypothesis, so you compare the p-value of 0.07 with a single-sided significance 
           level of 2*0.05 = 0.10, so we conclude that there is indeed a 
           statistically significant difference in favor of the UK."),
    answer("no", message = "Check the hypothesis again, what is the null hypothesis 
           in this case? Hint: it is NOT H0: difference between UK and US is zero."),
    answer("There is not enough info in this table"),
    random_answer_order = TRUE,
    allow_retry = TRUE
  )
)
```

### Diagnostics

```{r models}
amazon <- bootcamp::amazon
mod_intcp <- mod_intcp <- lm(after_sales ~ 1, data = amazon)
mod_2 <- lm(after_sales ~ before_sales + logo, data = amazon)
mod_3 <- lm(after_sales ~ before_sales + logo + country + age*sex, data = amazon)

amazon$change_sales <- amazon$after_sales - amazon$before_sales
mod_change <- lm(change_sales ~ logo + country + age*sex, data = amazon)
```

Let us run one final model:

```{r mod_3, exercise = TRUE, exercise.setup = "load_amazon"}
mod_3 <- lm(after_sales ~ before_sales + logo + country + age*sex, data = amazon)
summary(mod_3)
```

We now have three models that aim to explain `after_sales`:

``` r
mod_intcp <- mod_intcp <- lm(after_sales ~ 1, data = amazon)
mod_2 <- lm(after_sales ~ before_sales + logo, data = amazon)
mod_3 <- lm(after_sales ~ before_sales + logo + country + age*sex, data = amazon)
```

Let's look at the diagnostic plots for model `mod_3`.

```{r mod_3_plot, exercise = TRUE, exercise.setup = "models"}
plot(mod_3)
```

What can we conclude?

```{r plot_quiz}
quiz(
  question("What can we conclude from the 'Residual-vs-fitted' plot?",
    answer("Looks great", correct = TRUE, 
           message = "There is no pattern, this is what we want"),
    answer("This seems off, it is not OK"),
    allow_retry = TRUE
  ),
  question("What can we conclude from the 'Q-Q' plot?",
    answer("Looks great"),
    answer("This seems off, it is not OK", correct = TRUE,
           message = "Do you see how the disturbances move 
           away from the diagonal at the top-right and 
           bottom-left? It is not terrible, but we do need 
           some caution."),
    allow_retry = TRUE
  ),
  question("What can we conclude from the 'Scale-Location' plot?",
    answer("Looks great", correct = TRUE, 
           message = "We are dealing with two groups of
           points, because of the categorical X's. 
           This makes it hard to interpret. It *may* be 
           that the variance is lower on the right hand of 
           the plot than on the left, but this may just 
           be an illusion. Anyway, something to look out 
           for, but overall no reason for much alarm."),
    answer("This seems off, it is not OK"),
    allow_retry = TRUE
  ),
  question("What can we conclude from the 'Residual-vs-leverage' plot?",
    answer("Looks great", correct = TRUE, 
           message = "We don't even see any Cook's distance 
           lines in the plot. All is just dandy."),
    answer("This seems off, it is not OK"),
    allow_retry = TRUE
  )
)
```

### Comparison

We have a few models and there are two ways to compare them.

1.  First, and this is often the best way--although data scientists tend to find
    it not cool--is to look at the models from the viewpoint of model
    interpretability and field expertise.<br> This makes us prefer `mod_2`,
    because it is easy to understand and the additional variables in `mod_3`
    don't seem to do much anyway.

2.  Second, we can quantitatively compare the models.

One way to do this is to use the `bootcamp::compareLM` function, which shows you
a bunch of fit measures to compare. Let's do this:

```{r mod_3_compareLM, exercise = TRUE, exercise.setup = "models"}
bootcamp::compareLM(mod_2, mod_3)
```

We did not add `mod_intcp`, because that is a so-called *empty model* that can't
be compared this way (and, as a result, causes an error if you attempt the
comparison anyway).

As you see, there is little that distinguishes these two models. You want the
AIC and BIC to be as low as possible, where the BIC is slightly to be preferred
over AIC, as it favors smaller models. The BIC prefers `mod_2` over `mod_3`, but
the difference in BIC is very small.

If you would be interested in pure fit to the data, then `mod_3` actually does
slightly better than `mod_3`. See: the $R^2$ (`R.squared`) and the adjusted
$R^2$ (`Adj.R.sq`).

Which indicator do you go for? Well, that depends on your research question and
objective and can differ between projects. My personal preference would be model
2, since that is the simpler model and, hence, the model that is most easily
interpreted and converted into practical actions. But it can well be that the
project you are conducting requires a different model selection approach.

### Road ahead

That was nice work. Does your head hurt? Great, it means you are learning
something! `r smilebeam(height = 2)`

## Standardization

You probably want to do some analyses of your own, without me telling you which
variables to include. We are close to doing just that.

Before we go there, however, we need to discuss one more quick topic with you:
*standardized* versus *unstandardized* corefficients. In the models we ran so
far, we considered the raw results, these are called the *unstandardized*
coefficients. Let's see why this matters in a quick example.

Below we use the *centrality* dataset (get more details using
`?bootcamp::centrality`)

```{r lm1, echo = TRUE}
# ?bootcamp::centrality
data(centrality, package = "bootcamp")
mod = lm(advice ~ race + education, data = centrality)
summary(mod)
```

In this model, we model how popular people are as advice-givers: are people
asked more for advice based on their race and their level of education? As you
can see, both are statistically significant for very low levels of $\alpha$ . We
also see that race has a negative effect. Because $race$ is coded as 0 for
"white" and 1 for "non-white," we see that "non-white" persons are less
frequently asked for advice. Education level, on the other hand, positively
affects a person's popularity to be approached for advice.

Now, let's look at the coefficients themselves. You see that the coefficient for
$race$ is slightly larger than that of $education$ (in absolute value), so it
appears that a person's race may be more important for whom you ask for advice
than that person's level of education. OK, that's clear, right?

But, wait a second: $race$ is a dummy, so it can only be 0 or 1, where level of
education runs from 1 to 6. So, 1 unit of increase in $race$ (= being non-white
rather than white) changes average $advice$ popularity about 0.93, while 1 unit
of increase in $education$ increases average $advice$ popularity 0.81 . But,
$education$ can go up all the way to 6, so overall it has much more potential to
affect $advice$ popularity than race which can only go from 0 to 1!

We can correct for that by scaling the variables so they are comparable in
scale: this is called *standardization*. Standardized variables are calculated
by subtracting their mean and dividing by their standard deviation for each
observation--statisticians call this the variable's *Z*-score. It ensures each
variable has mean 0 and standard deviation 1. Mathematically, for each
observation $j$ of the variable X, we calculate the Z-score using the formula:

![](https://quantifyinghealth.com/wp-content/uploads/2020/04/z-score-formula.png){alt="z-score formula"}

When you run the regression model with these standardized variables, the
estimated coefficients are called *standardized*. Luckily, you only need to
estimate the model the way you are used to and can then calculate the
standardized coefficients from it. This is done by the `bootcamp::add_beta`
function from your favorite statistics package. This reproduces the results
table, but now with the standardized coefficients added.

```{r lm_std, echo = TRUE}
bootcamp::add_beta(mod)
```

This does not change statistical significance, the only thing that changes is
the variable's coefficient. Unfortunately, this does screw up the interpretation
of the coefficients--you now interpret the results as follow: an increase of 1
standard deviation in $education$ is associated with an average increase of
0.367 standard deviations in $advice$ popularity. Not an easy thing to wrap your
head around. But the good news is that the coefficients are now comparable
because they all have the same scale. Now, you see what we already concluded:
education indeed matters more than race (0.36 is larger than 0.15).

Side note: of course, in an actual empirical analysis, you want to test whether
this difference between the coefficients is statistically significant, but that
is not the objective of today's tutorial. Also, there is an active debate in the
statistics community whether one should standardize dummy variables (I
personally am not a fan). But again, no need to discuss that here.

[The important point is:]{.underline}

-   run your normal regression model and interpret the unstandardized
    coefficients substantively. Make sure you understand how these variables are
    measured and scaled.

-   look at the standardize coefficients to eye-ball which variable matters more
    than others

## Unguided analysis

Now, you are ready for the real work. Show me your raw statistical power!

![](images/powerlift.jpg)

### The dataset

The dataset we will use is the `centrality` dataset from the `bootcamp` package.

This is a dataset where researcher were interested in which personal
characterics make one person get to a more central position in a network in a
company than other people.

The dataset contains data on 850 employees of a company and includes their

-   demographics: *education*, *race*, *gender*, *age*

-   Big Five personality traits OCEAN (see here [in
    English](https://en.wikipedia.org/wiki/Big_Five_personality_traits) or [in
    Dutch](https://nl.wikipedia.org/wiki/Big_five_(persoonlijkheidstrekken)) for
    more detail on these traits, if you are interested)

-   the personality trait *activity_preference*

-   measures of the extent to which they are surrounded by colleagues of the
    same gender or race as themselves

Furthermore, three centrality scores are in the data: advice, friendship, and
adversion. The centralities are measured through *indegree* (don't worry if this
term doesn't mean anything to you, you'll learn about this in the SNA4DS
course). These variables measure how many people go to these persons to get
advice ($advice$) / how many people consider a person as a friend ($friendship$)
/ how many people dislike a person ($adverse$).

HERE IS EXERCISE ***ONE***:

::: infobox
1.  Start with loading the data and exploring it (I suggest you do this inside
    RStudio, not inside this tutorial window).

2.  Now, pick one of the centralities: `advice`, `friendship`, or `adverse`and
    use that as the dependent variable you want to explain.

3.  Build several regression models, with varying levels of complexity.<br>
    Experiment with interactions and transformations, so you become really good
    at doing this in `r rproj()`.<br> That will only benefit you.

4.  Look at the diagnostic plots to check for anything fishy.

5.  Compare multiple models based on theory or field expertise and compare the
    models based on quantitative measures.

6.  Really take the time to play with the data and with the modeling. Future-You
    will thank Present-You for it, as you will learn a fluency in `r rproj()`
    that will benefit you for the rest of the week, the rest of the semester,
    and many years to come.

7.  Ask for help if you run into a challenge you can't resolve.

8.  Have any further questions, comments, and stories to share in tomorrow's
    feedback session.

9.  And, most importantly, ENJOY!
:::

<br>

And, to give you enough practice, HERE IS EXERCISE ***TWO***:

::: infobox
1.  Load the *fifa22* dataset*:* `data(fifa22, package = "bootcamp")`. This is a
    dataset with data about value (*value_eur*) of soccer players from the
    FIFA22 game. In addition, there are several dozens of variables with data
    about the skill level and characteristics of these players.

2.  Explore the data.

3.  Pick either a player's estimated market value (*value_eur*) or wage
    (*wage_eur*) and model it as a function of player variables. For example,
    assess how the position on the pitch matters (do attackers get paid more
    than defenders?), what the effect is of how long a player's contract still
    runs, and/or how a player's mentality affects his market value/wage. Perhaps
    there are interactions between some of the predictors?

4.  Build several regression models, with varying levels of complexity.<br>
    Experiment with interactions and transformations, so you become really good
    at doing this in `r rproj()`.<br> That will only benefit you.

5.  Look at the diagnostic plots to check for anything fishy.

6.  Compare multiple models based on theory or field expertise and compare the
    models based on quantitative measures.

7.  Really take the time to play with the data and with the modeling. Future-You
    will thank Present-You for it, as you will learn a fluency in `r rproj()`
    that will benefit you for the rest of the week, the rest of the semester,
    and many years to come.

8.  Ask for help if you run into a challenge you can't resolve.

9.  Have any further questions, comments, and stories to share in tomorrow's
    feedback session.

10. And, most importantly, ENJOY!
:::

<br><br><br>


Oh, and here's some bonus exercises. For fun. And educational too.
Many people mistakingly believe that OLS models only fit straight lines. 
That is nonsense! OLS models are linearly additive, but that doesn't mean that 
the resulting relationships between `x` and `y` should be linear!

Here are 11 plots, of `x` against `y`. The plotted line simply is $y = f(x)$, 
it doesn't get more linear than that.
Can you figure out what the $f(x)$ is, for each plot? They are all very simple.

::: infobox
```{r, plotA, echo - FALSE}
x <- seq(-10, 10, length = 100)
plot(x, 3*x^2, type = "l", main = "Plot A", ylab = "y")
```

```{r, plotB, echo - FALSE}
x <- seq(-10, 10, length = 100)
plot(x, -3*x^2, type = "l", main = "Plot B", ylab = "y")
```

```{r, plotC, echo - FALSE}
x <- seq(-10, 10, length = 100)
suppressWarnings(plot(x, log(x), type = "l", main = "Plot C", ylab = "y"))
```

```{r, plotD, echo - FALSE}
x <- seq(-10, 10, length = 100)
suppressWarnings(plot(x, -log(x), type = "l", main = "Plot D", ylab = "y"))
```

```{r, plotE, echo - FALSE}
x <- seq(-10, 10, length = 100)
plot(x, 1/(3*x^2), type = "l", main = "Plot E", ylab = "y")
```

```{r, plotF, echo - FALSE}
x <- seq(-10, 10, length = 100)
plot(x, 1/(-3*x^2), type = "l", main = "Plot F", ylab = "y")
```

```{r, plotG, echo - FALSE}
x <- seq(-10, 10, length = 100)
plot(x, 3*x^3, type = "l", main = "Plot G", ylab = "y")
```

```{r, plotH, echo - FALSE}
x <- seq(-10, 10, length = 100)
plot(x, exp(x), type = "l", main = "Plot H", ylab = "y")
```

```{r, plotI, echo - FALSE}
x <- seq(-10, 10, length = 100)
plot(x, -exp(x), type = "l", main = "Plot I", ylab = "y")
```

```{r, plotJ, echo - FALSE}
x <- seq(-10, 10, length = 100)
plot(x, 1/exp(x), type = "l", main = "Plot J", ylab = "y")
```

```{r, plotK, echo - FALSE}
x <- seq(-10, 10, length = 100)
plot(x, -1/exp(x), type = "l", main = "Plot K", ylab = "y")
```


:::

In real OLS's, you can have combinations of these, and that can represent quite 
funky and non-linear relations between $x$ and $y$!


<br><br><br>

> GO FOR IT AND EXPLORE!

![](images/lion.jpg){width="806"}
